# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
# renovate: datasource=docker depName=ghcr.io/siderolabs/installer
talosVersion: v1.11.1
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.34.0
# Cluster configuration
clusterName: proxmox-talos-prod-cluster
endpoint: https://${CONTROL_PLANE_ENDPOINT_IP}:6443
clusterPodNets:
- 10.14.0.0/16
clusterSvcNets:
- 10.15.0.0/16
# NOTE: Using pre-built Image Factory schematics, so dynamic schematic customization is removed.
# Non-GPU schematic ID: dc60c70fb44bd81b1bce5b3978a68a59c929f4a36395cea9df21a24c1fa1dd33
# GPU schematic ID:     274c5c6e739dbb359c1ad19c304a5c064d23f00baaa18c556bb460ee08483ab2
# Node configurations
nodes:
# Control plane node (from Terraform)
- hostname: ${TALOS_CONTROL_PLANE_NAME_0}
  controlPlane: true
  ipAddress: ${TALOS_CONTROL_PLANE_IP_0}
  installDisk: /dev/sda
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_CONTROL_PLANE_MAC_0}"
    dhcp: false
    addresses:
    - ${TALOS_CONTROL_PLANE_IP_0}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}

# Worker nodes (from Terraform)
- hostname: ${TALOS_WORKER_NAME_1}
  controlPlane: false
  ipAddress: ${TALOS_WORKER_IP_1}
  installDisk: /dev/sda
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_WORKER_MAC_1}"
    dhcp: false
    addresses:
    - ${TALOS_WORKER_IP_1}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}
  nodeLabels:
    node-type: worker

# Additional worker node (from Terraform)
- hostname: ${TALOS_WORKER_NAME_2}
  controlPlane: false
  ipAddress: ${TALOS_WORKER_IP_2}
  installDisk: /dev/sda
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_WORKER_MAC_2}"
    dhcp: false
    addresses:
    - ${TALOS_WORKER_IP_2}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}
  nodeLabels:
    node-type: worker

# GPU worker node (from Terraform)
- hostname: ${TALOS_GPU_WORKER_NAME_0}
  controlPlane: false
  ipAddress: ${TALOS_GPU_WORKER_IP_0}
  installDisk: /dev/sda
  # GPU node uses a different pre-built schematic (LTS Nvidia extensions)
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_GPU_WORKER_MAC_0}"
    dhcp: false
    addresses:
    - ${TALOS_GPU_WORKER_IP_0}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}
  nodeLabels:
    node-type: gpu-worker
  patches:
  - |-
    machine:
      kernel:
        modules:
          - name: nvidia
          - name: nvidia_uvm
          - name: nvidia_drm
          - name: nvidia_modeset
      files:
        - path: /etc/cri/conf.d/20-customization.part
          op: create
          content: |
            [plugins]
              [plugins."io.containerd.cri.v1.runtime"]
                [plugins."io.containerd.cri.v1.runtime".containerd]
                  default_runtime_name = "nvidia"
  # Pin GPU schematic image so NVIDIA modules/toolkit are present
  - |-
    machine:
      install:
        image: factory.talos.dev/metal-installer/274c5c6e739dbb359c1ad19c304a5c064d23f00baaa18c556bb460ee08483ab2:v1.11.0
# Global patches
patches:
# Network configuration
- |-
  machine:
    network:
      nameservers:
        - 1.1.1.1
        - 1.0.0.1
# Time configuration
- |-
  machine:
    time:
      disabled: false
      servers:
        - time.cloudflare.com
# Common kernel modules and sysctls
- |-
  machine:
    sysctls:
      vm.nr_hugepages: "1024"
      net.core.bpf_jit_harden: 1
    kernel:
      modules:
        - name: nvme_tcp
        - name: vfio_pci
        - name: uio_pci_generic
# Control plane specific configuration
controlPlane:
  patches:
  # Default containerd runtime for control plane nodes
  - |-
    machine:
      files:
        - path: /etc/cri/conf.d/20-customization.part
          op: create
          content: |
            [plugins]
              [plugins."io.containerd.cri.v1.runtime"]
                [plugins."io.containerd.cri.v1.runtime".containerd]
                  default_runtime_name = "runc"
  # Cluster configuration
  - |-
    cluster:
      controllerManager:
        extraArgs:
          bind-address: 0.0.0.0
      proxy:
        disabled: true
      scheduler:
        extraArgs:
          bind-address: 0.0.0.0
  # CNI configuration
  - |-
    cluster:
      network:
        cni:
          name: none
  # Node labels for control plane nodes that should not be load balancers
  - |-
    machine:
      nodeLabels:
        node.kubernetes.io/exclude-from-external-load-balancers: ""
  # Explicitly pin control plane image (same as non-GPU workers)
  - |-
    machine:
      install:
        image: factory.talos.dev/metal-installer/dc60c70fb44bd81b1bce5b3978a68a59c929f4a36395cea9df21a24c1fa1dd33:v1.11.0
# Worker specific configuration
worker:
  patches:
  # Pin non-GPU worker image (GPU node has its own node-level override)
  - |-
    machine:
      install:
        image: factory.talos.dev/metal-installer/dc60c70fb44bd81b1bce5b3978a68a59c929f4a36395cea9df21a24c1fa1dd33:v1.11.0
  # Longhorn storage configuration
  - |-
    machine:
      kubelet:
        extraMounts:
          - destination: /var/lib/longhorn
            type: bind
            source: /var/lib/longhorn
            options:
              - bind
              - rshared
              - rw
      disks:
        - device: /dev/sdb
          partitions:
            - mountpoint: /var/mnt/longhorn_sdb
  # (Optional future) Override GPU node image separately; applied via node-specific patch if needed.

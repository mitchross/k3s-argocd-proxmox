apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
  namespace: llama-cpp
data:
  config.yaml: |
    # Llama Swap configuration optimized for dual RTX 3090s (48GB VRAM total)
    # Qwen3-Coder-30B-A3B-Instruct supports native 256K context length
    models:
      # Model 1: Qwen3 Coder Q4_K_M - 32K context for faster inference
      qwen3-coder-q4-32k:
        cmd: "llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf -c 32768 -ngl 99 --host 0.0.0.0 --port ${PORT} --threads 8 --threads-batch 8 --tensor-split 0.5,0.5 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 2: Qwen3 Coder Q5_K_M - 32K context with higher quality
      qwen3-coder-q5-32k:
        cmd: "llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf -c 32768 -ngl 99 --host 0.0.0.0 --port ${PORT} --threads 8 --threads-batch 8 --tensor-split 0.5,0.5 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 3: Qwen3 Coder Q4_K_M - Full 256K context for repository-scale understanding
      qwen3-coder-q4-256k:
        cmd: "llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf -c 262144 -ngl 99 --host 0.0.0.0 --port ${PORT} --threads 8 --threads-batch 8 --tensor-split 0.5,0.5 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 900s

      # Model 4: Qwen3 Coder Q5_K_M - Full 256K context with highest quality
      qwen3-coder-q5-256k:
        cmd: "llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf -c 262144 -ngl 99 --host 0.0.0.0 --port ${PORT} --threads 8 --threads-batch 8 --tensor-split 0.5,0.5 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 900s
        
    # Global settings optimized for Qwen3-Coder
    default_model: "qwen3-coder-q5-32k"
    health_check_interval: "30s"
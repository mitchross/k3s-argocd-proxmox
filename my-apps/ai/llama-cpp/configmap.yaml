apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
  namespace: llama-cpp
data:
  config.yaml: |
    # Llama Swap configuration optimized for Qwen3-Coder with dual RTX 3090s (48GB VRAM total)
    models:
      # Model 1: Qwen3 Coder Q4_K_M - 80K context, optimal for dual 3090s
      qwen3-coder-q4-80k:
        cmd: "/app/llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf -c 81920 -ngl 99 --split-mode layer --tensor-split 1,1 --flash-attn --host 0.0.0.0 --port ${PORT} --threads 16 --threads-batch 8 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 2: Qwen3 Coder Q4_K_M - 64K context, balanced performance
      qwen3-coder-q4-64k:
        cmd: "/app/llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf -c 65536 -ngl 99 --split-mode layer --tensor-split 1,1 --flash-attn --host 0.0.0.0 --port ${PORT} --threads 16 --threads-batch 8 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 3: Qwen3 Coder Q5_K_M - 64K context, higher quality
      qwen3-coder-q5-64k:
        cmd: "/app/llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf -c 65536 -ngl 99 --split-mode layer --tensor-split 1,1 --flash-attn --host 0.0.0.0 --port ${PORT} --threads 16 --threads-batch 8 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 4: Qwen3 Coder Q5_K_M - 48K context, safe for memory
      qwen3-coder-q5-48k:
        cmd: "/app/llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf -c 49152 -ngl 99 --split-mode layer --tensor-split 1,1 --flash-attn --host 0.0.0.0 --port ${PORT} --threads 16 --threads-batch 8 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 5: Qwen3 Coder Q8_K_XL - 32K context, maximum quality
      qwen3-coder-q8-32k:
        cmd: "/app/llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf -c 32768 -ngl 99 --split-mode layer --tensor-split 1,1 --flash-attn --host 0.0.0.0 --port ${PORT} --threads 16 --threads-batch 8 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

      # Model 6: Qwen3 Coder Q8_K_XL - 24K context, very conservative
      qwen3-coder-q8-24k:
        cmd: "/app/llama-server -m /models/Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf -c 24576 -ngl 99 --split-mode layer --tensor-split 1,1 --flash-attn --host 0.0.0.0 --port ${PORT} --threads 16 --threads-batch 8 --temp 0.7 --top-p 0.8 --top-k 20 --repeat-penalty 1.05"
        health_check_path: "/health"
        timeout: 600s

    # Global settings
    default_model: "qwen3-coder-q4-80k"
    health_check_interval: "30s"
    swap_timeout: "120s"
    cleanup_timeout: "60s"